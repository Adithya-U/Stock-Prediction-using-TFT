{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":927894,"sourceType":"datasetVersion","datasetId":500872},{"sourceId":11267974,"sourceType":"datasetVersion","datasetId":7043532}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Load Tesla news dataset - using read_excel for .xlsx files\nnews_df = pd.read_excel(\n    \"/kaggle/input/tesla-news-2010-2020/tesla_news_articles_2010_2020.xlsx\",\n    dtype={\n        \"title\": str,\n        \"snippet\": str,\n        \"lead_paragraph\": str,\n        \"keywords\": str,\n        \"section\": str,\n        \"source\": str,\n        \"url\": str\n    }\n).fillna('')  # Replace NaN with empty strings\n\n# Convert date column to datetime format\nnews_df[\"date\"] = pd.to_datetime(news_df[\"date\"])\n\n# Sort by date\nnews_df = news_df.sort_values(\"date\")\n\n# Group by date (concatenating text fields)\nnews_df = news_df.groupby(\"date\").agg({\n    \"title\": lambda x: \" | \".join(x),\n    \"snippet\": lambda x: \" | \".join(x),\n    \"lead_paragraph\": lambda x: \" | \".join(x),\n    \"keywords\": lambda x: \", \".join(x),\n    \"section\": lambda x: \", \".join(x),\n    \"source\": lambda x: \", \".join(x),\n    \"url\": lambda x: \", \".join(x)\n}).reset_index()\n\n# Forward-fill missing dates\nstock_dates = pd.date_range(start=news_df[\"date\"].min(), end=news_df[\"date\"].max(), freq=\"D\")\nnews_df = news_df.set_index(\"date\").reindex(stock_dates).fillna(method=\"ffill\").reset_index()\nnews_df.rename(columns={\"index\": \"date\"}, inplace=True)\n\n# Save the processed dataset\nnews_df.to_csv(\"tesla_news_processed.csv\", index=False)\n\nprint(f\"Preprocessed dataset saved with {len(news_df)} rows\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load stock dataset to get valid trading dates\nstock_df = pd.read_csv(\"/kaggle/input/tesla-stock-data-from-2010-to-2020/TSLA.csv\")  \nstock_df[\"Date\"] = pd.to_datetime(stock_df[\"Date\"])\nvalid_trading_dates = set(stock_df[\"Date\"])\n\n# Filter news to keep only trading days\nnews_df = news_df[news_df[\"date\"].isin(valid_trading_dates)]\n\n# Save the corrected dataset\nnews_df.to_csv(\"tesla_news_filtered.csv\", index=False)\n\nprint(f\"Filtered dataset saved with {len(news_df)} rows (matching stock trading days)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\n# Load preprocessed Tesla news dataset\nnews_df = pd.read_csv(\"tesla_news_filtered.csv\")\nnews_df[\"date\"] = pd.to_datetime(news_df[\"date\"])\n\n# Concatenate text fields\nnews_df[\"full_text\"] = news_df[\"title\"] + \" \" + news_df[\"snippet\"] + \" \" + news_df[\"lead_paragraph\"]\n\n# Load pre-trained sentence-transformers model (e.g., \"all-MiniLM-L6-v2\")\nembed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Compute embeddings\nnews_df[\"embedding\"] = news_df[\"full_text\"].apply(lambda text: embed_model.encode(text, convert_to_numpy=True))\n\n# Convert list of arrays to a 2D NumPy array\nembedding_matrix = np.vstack(news_df[\"embedding\"].values)\n\n# Save embeddings to CSV\nnews_embeddings_df = pd.DataFrame(embedding_matrix)\nnews_embeddings_df.insert(0, \"date\", news_df[\"date\"])  # Insert date column\nnews_embeddings_df.to_csv(\"news_embeddings.csv\", index=False)\n\nprint(f\"News embeddings saved with shape: {embedding_matrix.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n# Load news embeddings dataset\nnews_embeddings_df = pd.read_csv(\"news_embeddings.csv\")\nnews_embeddings_df[\"date\"] = pd.to_datetime(news_embeddings_df[\"date\"])\n\n# Extract only the embedding columns (assume embeddings start from column index 1)\nembedding_matrix = news_embeddings_df.iloc[:, 1:].values  # Exclude date column\n\n# Apply PCA to reduce embeddings from 384 to 50 dimensions\npca = PCA(n_components=5)\nreduced_embeddings = pca.fit_transform(embedding_matrix)\n\n# Convert back to DataFrame\nreduced_embeddings_df = pd.DataFrame(reduced_embeddings, columns=[f\"pca_{i+1}\" for i in range(5)])\nreduced_embeddings_df.insert(0, \"date\", news_embeddings_df[\"date\"])  # Reinsert date column\n\n# Save reduced embeddings\nreduced_embeddings_df.to_csv(\"news_embeddings_pca.csv\", index=False)\n\nprint(f\"Reduced news embeddings saved with shape: {reduced_embeddings_df.shape}\")\nimport pandas as pd\n\n# Load stock dataset\nstock_df = pd.read_csv(\"/kaggle/input/tesla-stock-data-from-2010-to-2020/TSLA.csv\")\nstock_df[\"Date\"] = pd.to_datetime(stock_df[\"Date\"])\n\n# Load reduced news embeddings dataset\nnews_embeddings_df = pd.read_csv(\"news_embeddings_pca.csv\")\nnews_embeddings_df[\"date\"] = pd.to_datetime(news_embeddings_df[\"date\"])\n\n# Merge stock prices with reduced news embeddings on date\nmerged_df = pd.merge(stock_df, news_embeddings_df, left_on=\"Date\", right_on=\"date\", how=\"left\").drop(columns=[\"date\"])\n\n# Save the final dataset\nmerged_df.to_csv(\"tesla_stock_news_fused_pca.csv\", index=False)\n\nprint(f\"Merged dataset saved with {len(merged_df)} rows and {merged_df.shape[1]} columns\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load stock dataset\nstock_df = pd.read_csv(\"/kaggle/input/tesla-stock-data-from-2010-to-2020/TSLA.csv\")\nstock_df[\"Date\"] = pd.to_datetime(stock_df[\"Date\"])\n\n# Load reduced news embeddings dataset\nnews_embeddings_df = pd.read_csv(\"news_embeddings_pca.csv\")\nnews_embeddings_df[\"date\"] = pd.to_datetime(news_embeddings_df[\"date\"])\n\n# Merge stock prices with reduced news embeddings on date\nmerged_df = pd.merge(stock_df, news_embeddings_df, left_on=\"Date\", right_on=\"date\", how=\"left\").drop(columns=[\"date\"])\n\n# Save the final dataset\nmerged_df.to_csv(\"tesla_stock_news_fused_pca.csv\", index=False)\n\nprint(f\"Merged dataset saved with {len(merged_df)} rows and {merged_df.shape[1]} columns\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/working/tesla_stock_news_fused_pca.csv')\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndf = df.sort_values(\"Date\").set_index(\"Date\")\n# Add critical columns\ndf[\"time_idx\"] = np.arange(len(df))  # Sequential integer index\ndf[\"group_id\"] = \"TSLA\"  # Single time series identifier\n\n# Create ALL required time features (FIX for KeyError)\ndf[\"Day_of_week\"] = df.index.dayofweek  # Monday=0, Sunday=6\ndf[\"Month\"] = df.index.month  # 1-12\ndf[\"Year\"] = df.index.year  # 2010-2020\ndf[\"Day_of_month\"] = df.index.day  # 1-31\ndf[\"Week_of_year\"] = df.index.isocalendar().week  # 1-52\n\n# Target: log returns (better than raw prices)\ndf[\"Target\"] = np.log(df[\"Close\"]).diff()  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.columns.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Keep only essential features\nkeep_cols = [\"Target\", \"Close\", \"Volume\", \"time_idx\", \"group_id\", \n             \"Day_of_week\", \"Month\", \"Year\", \"Day_of_month\", \"Week_of_year\",\"pca_1\",\"pca_2\",\n             \"pca_3\",\"pca_4\",\"pca_5\"]\ndf = df[keep_cols].dropna()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code]\n# Train/validation/test split\ntest_cutoff = df.index[-365]  # Last year for testing\nval_cutoff = test_cutoff - pd.Timedelta(days=365)  # Year before test\n\ntrain_df = df.loc[df.index < val_cutoff]\nval_df = df.loc[(df.index >= val_cutoff) & (df.index < test_cutoff)]\ntest_df = df.loc[df.index >= test_cutoff]\n\n# %% [code]\n# Create dataset\nmax_encoder_length = 60  # Lookback window\nmax_prediction_length = 7  # Forecast horizon\n\ntraining = TimeSeriesDataSet(\n    train_df,\n    time_idx=\"time_idx\",\n    target=\"Target\",\n    group_ids=[\"group_id\"],\n    max_encoder_length=max_encoder_length,\n    max_prediction_length=max_prediction_length,\n    time_varying_known_reals=[\"Day_of_week\", \"Month\", \"Year\", \"Day_of_month\", \"Week_of_year\"],\n    time_varying_unknown_reals=[\"Target\", \"Close\", \"Volume\",\"pca_1\", \"pca_2\", \"pca_3\", \"pca_4\", \"pca_5\"],\n    static_categoricals=[],\n    static_reals=[],\n    target_normalizer=None,\n    add_relative_time_idx=True,\n)\n\nvalidation = TimeSeriesDataSet.from_dataset(training, val_df, predict=True)\n# %% [code]\n# Create dataloaders\nbatch_size = 64\ntrain_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\nval_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code]\n# Configure TFT model\ntft = TemporalFusionTransformer.from_dataset(\n    training,\n    learning_rate=0.001,\n    hidden_size=16,\n    attention_head_size=1,\n    dropout=0.2,\n    hidden_continuous_size=8,\n    loss=QuantileLoss(),\n    optimizer=\"adamw\",\n)\n\n# %% [code]\n# Train with early stopping\ntrainer = pl.Trainer(\n    max_epochs=50,\n    accelerator=\"auto\",\n    enable_model_summary=True,\n    gradient_clip_val=0.1,\n    callbacks=[\n        pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5),\n        pl.callbacks.LearningRateMonitor()\n    ],\n)\n\ntrainer.fit(tft, train_dataloader, val_dataloader)\n\n# %% [code]\n# Evaluate on test set - CORRECTED VERSION\ntest_dataset = TimeSeriesDataSet.from_dataset(training, test_df, predict=True)\ntest_dataloader = test_dataset.to_dataloader(train=False, batch_size=batch_size, num_workers=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code]\n# Corrected Evaluation and Prediction Conversion\n\n# Get predictions - using the right mode\npredictions = tft.predict(test_dataloader, mode=\"raw\")\npoint_predictions = tft.to_prediction(predictions)  # Gets median prediction (quantile=0.5)\n\n# Convert to numpy array and flatten\npredicted_returns = point_predictions.cpu().numpy().flatten()\n\n# Calculate available test points\ntest_points = len(predicted_returns)\n\n# Verify shapes\nprint(f\"Predicted returns shape: {predicted_returns.shape}\")\nprint(f\"Available test points: {test_points}\")\n\n# Initialize price conversion\npredicted_prices = np.zeros(test_points)\ncurrent_price = test_df[\"Close\"].iloc[max_encoder_length-1]  # Last known price\n\n# Walk forward conversion\nfor i in range(test_points):\n    predicted_prices[i] = current_price * np.exp(predicted_returns[i])\n    current_price = predicted_prices[i]  # Update for next step\n\n# Get corresponding actual prices\nactual_prices = test_df[\"Close\"].iloc[max_encoder_length:max_encoder_length+test_points].values\n\n# %% [code]\n# Calculate metrics\nmae = np.mean(np.abs(predicted_prices - actual_prices))\nrmse = np.sqrt(np.mean((predicted_prices - actual_prices)**2))\nmape = np.mean(np.abs((actual_prices - predicted_prices)/actual_prices)) * 100\n\nprint(f\"\\nEvaluation Metrics:\")\nprint(f\"MAE: ${mae:.2f}\")\nprint(f\"RMSE: ${rmse:.2f}\")\nprint(f\"MAPE: {mape:.2f}%\")\nprint(f\"Test Period: {test_df.index[max_encoder_length].date()} to {test_df.index[-1].date()}\")\nprint(f\"Test Points: {test_points} days\")\n\n# %% [code]\n# Enhanced Plotting\nplt.figure(figsize=(14, 7))\nplt.plot(test_df.index[max_encoder_length:max_encoder_length+test_points], \n         actual_prices, 'b-', label=\"Actual Price\")\nplt.plot(test_df.index[max_encoder_length:max_encoder_length+test_points], \n         predicted_prices, 'r--', label=\"Predicted Price\")\nplt.fill_between(test_df.index[max_encoder_length:max_encoder_length+test_points],\n                actual_prices * 0.98, actual_prices * 1.02,\n                color='gray', alpha=0.2, label=\"Â±2% Range\")\nplt.title(f\"Tesla Stock Price Prediction\\n{test_points}-Day Test Period\", fontsize=14)\nplt.xlabel(\"Date\", fontsize=12)\nplt.ylabel(\"Price ($)\", fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}